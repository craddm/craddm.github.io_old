---
title: "Ratcliff RT sims"
output: html_notebook
---


```{r loadPackages, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(afex)
library(plotly)
```

Reaction times are a very common outcome measure in psychological science. Frqeuently, people use the mean to summarise reaction time distributions and compares means across conditions using ANOVAs. For example, in a typical experiment, researchers might record reaction times to familiar and unfamiliar faces, and look for differences in mean reaction time across these two types of stimuli.

An issue with this is that reaction time distributions are skewed: there are many more short values than long values, so their distribution has a long right tail. The mean of such distributions is drawn out towards the tail. Unlike in normally distributed data, it becomes quite different from the median, which is the midpoint of the distribution. Thus the mean is influenced by the skewness of the distribution in a way that the median is not. In addition, the median is robust to outliers.

A classic article from 1993 by Ratcliff[1] examined how outliers in reaction time distributions impacted statistical power, and how a variety of different methods can be used to mitigate their influence. I'm going to replicate some of his simulations here.

RTs can be modelled as coming from an ex-Gaussian distribution. An ex-Gaussian (which isn't pining for the fjords) is the sum of a Gaussian and an exponential distribution, and has three parameters - the mean of the Gaussian (mu), standard deviation of the Gaussian (sigma), and the mean of the exponential (tau). Between them, sigma and tau effectively control the rise of the left tail of the distribution and the fall of the right tail. Let's simulate an ex-Gaussian by generating a normal distribution with a mean of 400 and SD of 40, an exponential distribution with a mean of 200 (this is given by a rate parameter in R's rexp command - divide 1 by your desired mean.)

```{r exGauss, message = FALSE}
exGauss <- data.frame(RT = rnorm(1000,400,40)+rexp(1000,1/200))
p<- ggplot(exGauss,aes(RT))+geom_histogram()+theme_classic()
ggplotly(p)

```

As always, other things come into play during psychology experiments. People get bored or press buttons too quickly. Our distributions become contaminated with outliers. Ratcliff simulated this by replacing values from the sample distribution with values from a second distribution. Note that this is outliers in the sense of datapoints drawn from a distribution other than that related to the process of interest. In fact, such outliers can easily by embedded unnoticed within the distribution generated by the genuine process, and are as such impossible to detect. Thus, in general, it's only the really short or really long outliers that can be identified as being outliers.

R has functions built-in to generate data from a lot of distributions, but not ex-Gaussians. So first of all I create a helper function to produce ex-Gaussian distributions with specified parameters - mu, sd, and tau. Here's a bunch of distributions produced by combining two distributions: a reference ex-Gaussian distribution which exemplifies the process of interest, and an outlier ex-Gaussian distribution with a mean 1 or 2 standard deviations away from the mean of the reference distribution, as in Ratcliff (1993).

```{r exGfun, message = FALSE}
# define a helper function to generate an ex-gaussian

exGausDist <- function(nObs = 1000,mu = 400, sd = 40, tau = 200) {
  round(rnorm(nObs,mu,sd)+rexp(nObs,1/tau))
}

# Generate the various distributions + outliers
exampleDists <- data.frame("Reference" = exGausDist(),
                           "Minus 1 Sd" = c(exGausDist(nObs = 800),exGausDist(nObs = 200,mu = 200)),
                           "Plus 1 Sd" = c(exGausDist(nObs = 800),exGausDist(nObs = 200,mu = 600)),
                           "Plus 2 Sd" = c(exGausDist(nObs = 800),exGausDist(nObs = 200,mu = 800)))

gg2 <- exampleDists %>% gather(distribution,RT) %>%
  ggplot(aes(x = RT))+
  geom_histogram(binwidth = 50)+
  facet_wrap(~distribution)+
  theme_bw()

ggplotly(gg2)

```

Note how with the "Plus 1 SD" distribution, the outliers are quite tricky to tell apart from the the genuine process - there's a much more noticeable bump in the tail of the "Plus 2 SD" plot, and an early bump in the "Minus 1 SD" plot.

# Ratcliff-style experimental simulations

Ratcliff (1993) ran simulations testing how various methods of dealing with outliers stood up. To generate RT distributions, for his simulations, Ratcliff created distributions in which 90% of the values were drawn directly from the genuine ex-Gaussian distribution, and 10% drawn from the same distribution but with a random number between 0 and 2000, drawn from a uniform distribution, added to it. Ratcliff also introduced inter-subject variability by vary individual subject means by a random number between -50 and 50, again drawn from a uniform distribution [2].

Let's start by building a helper function to generate data distributions with a specified proportion of outliers. I then create a second function to generate a full set of experimental data as per Ratcliff. This generates data for a 2 X 2 design with within-subjects factors A and B. Ratcliff generated data for 32 subjects, with 7 trials in each condition (more on that later), an ex-Gaussian distribution with parameters mu = 400, sigma = 40, and tau = 200, and a main effect in factor A, so let's make sure we can vary all those things at will.

```{r ratcliffData, message = FALSE}

exGausDist <- function(nObs = 1000, mu = 400, sd = 40, tau = 200, outProb = .1) {
  #generate vector of trials
  tmp <- round(rnorm(nObs, mu, sd)+rexp(nObs, 1 / tau))
  #pick the lucky ones
  rollD <- rbinom(nObs, 1, prob = outProb) 
  outliers <- round(runif(nObs, 0, 2000))
  #add the outliers to the genuine distribution
  tmp[rollD] <- tmp[rollD]+outliers[rollD]
  tmp
}

## Custom function to generate a Ratcliff style dataset
## Main effect is always in A
ratcliff <- function(nSubs = 32, mainEffect = 0, mu = 400, subjVar = 50, nObs = 7, outProb = 0, tau = 200, sigma = 40) {
  #generate a vector of individual subject means with variance drawn from a uniform distribution
  subMeans <- rep(mu,nSubs)+runif(nSubs,min = -subjVar, max = subjVar)
  
  #generate a full dataset
  ratcliff_data <- data.frame(A1.B1 = unlist(lapply(subMeans,
                                          function (x) do.call(exGausDist,
                                                               list(nObs = nObs, mu = x, sd = sigma,
                                                                    tau = tau, outProb = outProb)))),
                    A1.B2 = unlist(lapply(subMeans,
                                          function (x) do.call(exGausDist,
                                                               list(nObs = nObs, mu = x, sd = sigma,
                                                                    tau = tau, outProb = outProb)))),
                    A2.B1 = unlist(lapply(subMeans,
                                          function (x) do.call(exGausDist,
                                                               list(nObs = nObs, mu = x + mainEffect, sd = sigma,
                                                                    tau = tau, outProb = outProb)))),
                    A2.B2 = unlist(lapply(subMeans,
                                          function (x) do.call(exGausDist,
                                                               list(nObs = nObs, mu = x + mainEffect, sd = sigma,
                                                                    tau = tau, outProb = outProb)))),
                    Subject = factor(rep(1:nSubs, each = nObs)))
  
  ratcliff_data <- ratcliff_data %>% 
    gather(condition,RT,-Subject) %>%
    separate(condition,c("A","B"))
  ratcliff_data
}

ratcliff_data <- ratcliff()

ggplot(ratcliff_data,aes(RT))+
  geom_density(aes(fill = Subject),alpha = 0.5)+
  facet_grid(B~A)+
  theme_bw()
```

So now we have a function that can simulate Ratcliff's simulated datasets.

Ratcliff generated 1000 simulated datasets and ran a 2X2X32(!) ANOVA for each one using a variety of methods for dealing with outliers. This in itself seems a bit odd, as you wouldn't normally do that - you'd do a 2x2 repeated measures ANOVA, which is what I'll be doing.

Let's write yet another function: this one generates a dataset using the ratcliff function I wrote above and runs an ANOVA on it. We'll keep it simple to start off with stats that don't require use of cut-offs or trimming (it's easier for me to do those seperately). We'll check the untrimmed mean, the median, the mean after the inverse transformation (1/RT), and the mean after log transformation. I'll leave the code in for it to do the full ANOVA, for anyone wanting to borrow it, but it does slow down the sims considerably.

```{r}
runSims <- function(mainEffect = 0, outProb = 0, nObs = 7) {
  
  tmp <- ratcliff(mainEffect = mainEffect,outProb = outProb,nObs = nObs)
  
  zz <- tmp %>% 
    group_by(Subject,A,B) %>%
    summarise(meanRT = mean(RT),
              medianRT = median(RT),
              invTr = mean(1/RT),
              logTr = mean(log(RT))
              )
  
  ZZ <- zz %>%
    gather(measure, RT, -Subject,-A, -B) %>%
    split(.$measure) %>%
    map(~aov_ez("Subject","RT", data = . ,within = c("A","B")))
  
  
  pA <- map_dbl(ZZ %>% at_depth(1,"anova_table") %>% map("Pr(>F)"),1)
  pB <- map_dbl(ZZ %>% at_depth(1,"anova_table") %>% map("Pr(>F)"),2)
  pAxB <- map_dbl(ZZ %>% at_depth(1,"anova_table") %>% map("Pr(>F)"),3)
  return(list("pA" = pA,"pB" = pB,"pAxB" = pAxB))
}

nSims <- 1000
no_outliers <- replicate(nSims,runSims(mainEffect = 30))
Apow <- do.call(rbind,no_outliers[1,])
#Uncomment these lines if you want to check the main effect of B and AxB interaction
#Bpow <- do.call(rbind,out[2,]) 
#AxBpow <- do.call(rbind,out[3,])

no_outliers <- data.frame((Apow<.05)+0) %>% gather(measure,percent) %>% ggplot(aes(x = measure, y = percent))+stat_summary(fun.y = "mean", geom= "point") + ggtitle("30ms main effect in mu, no outliers")+theme_bw()

out <- replicate(nSims,runSims(mainEffect = 30,outProb = 0.05))

Apow <- do.call(rbind,out[1,])
Bpow <- do.call(rbind,out[2,])
AxBpow <- do.call(rbind,out[3,])

five_percent_outliers <- data.frame((Apow<.05)+0) %>% gather(measure,percent) %>% ggplot(aes(x = measure, y = percent))+stat_summary(fun.y = "mean", geom= "point")+ ggtitle("30ms main effect in mu, 5% outliers")+theme_bw()

ggplotly(no_outliers)
ggplotly(five_percent_outliers)

```




[2] A Gaussian distribution of subject variability would probably be more realistic, but more on that some other time.