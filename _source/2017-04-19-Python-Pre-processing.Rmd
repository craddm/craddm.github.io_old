---
title: 'Python in R and R in Python?'
comments: true
layout: post
output:
  html_document: default
  pdf_document: default
  word_document: default
categories:
- EEG
- ERPs
- R
- ggplot2
- Python
date: "2017-04-19"
image: "plotCleanVsNot-1.png"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As mentioned in my [last post](../blog/2017/04/07/EEG-in-R-what-is-missing/), an issue doing EEG analysis in R at the moment is that there's a distinct lack of tools in R for a lot of the typical processing steps. In the past I've done a lot of processing in Matlab (specifically with EEGLAB and Fieldtrip) and shifted things over to R for statistics. But all is not lost. For example, with the following code, I can run a bunch of preprocessing, including automatic artefact rejection, and have nice ERPs in R in the blink of an eye!

```{python, engine.path = "C:\\Users\\Matt\\Anaconda3\\envs\\NewPython\\python.exe", eval = FALSE}
import mne
import numpy as np
import pandas
from autoreject import (LocalAutoRejectCV, compute_thresholds,
                        set_matplotlib_defaults, plot_epochs)
from functools import partial

thresh_func = partial(compute_thresholds, method='random_search')
ar = LocalAutoRejectCV(thresh_func=thresh_func, verbose='progressbar')

eogChans = (64,65,66,67)
miscChans = (68,69,70,71)

subjects = []

file_list = []
for i in range(6):
    file_list += ['EXAMPLEDATA{0}.bdf'.format(i+1)]

unclean_epochs = []
for file in file_list):
    raw = mne.io.read_raw_edf(file,preload = True,eog = eogChans,misc = miscChans)
    raw.info['bads'] = ['EXG7','EXG8']
    picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, exclude='bads')
    events = mne.find_events(raw, stim_channel='STI 014')
    event_id = {'light': 100, 'no light': 128}
    tmin = -.3
    tmax = .6
    raw.filter(1, 45,l_trans_bandwidth = 1, h_trans_bandwidth = 5, filter_length = 'auto')
    montage = mne.channels.read_montage('biosemi64')
    raw.set_montage(montage)
    raw_avg_ref, _ = mne.io.set_eeg_reference(raw)
    epochs_avg_ref = mne.Epochs(raw_avg_ref, events=events, event_id=event_id, tmin=tmin,
                    tmax=tmax, picks=picks, add_eeg_ref=False,preload = True, detrend = None,decim = 4)
    unclean_epochs += [epochs_avg_ref]

preclean_epochs = mne.concatenate_epochs(unclean_epochs)
epochs_ar = ar.fit_transform(preclean_epochs.copy())

evoked_clean = epochs_ar.average()
evoked_preclean = preclean_epochs.average()
df = evoked_clean.to_data_frame()
df.to_csv('EXAMPLEDATA\\evoked_clean.csv')
df = evoked_preclean.to_data_frame()
df.to_csv('EXAMPLEDATA\\evoked_preclean.csv')
```

The eagle-eyed amongst you have probably spotted something unusual about this code. No, not that it's bad. All my code is bad, there's nothing unusual about that. It's not R code. It's Python. I wrote the above code a few months ago when I was just starting out with it. It uses the [MNE](https://martinos.org/mne/stable/index.html) package to load the raw data, run [automatic artefact rejection](http://autoreject.github.io/), and save both the cleaned and unclean data as CSVs.

And yet, even though it's Python code, I ran it directly in an R Notebook.

Normally when you set up a code chunk in an R Notebook, you add something like this (without the blocks):
<pre>
```{r} 
Add your code here.
```
</pre>
But a fun thing about R Notebooks is that they're written in R Markdown, and R Markdown separates your code into individually executable chunks. Those chunks can run code from a variety of [different languages](http://rmarkdown.rstudio.com/authoring_knitr_engines.html) when you [knit](https://yihui.name/knitr/) the document together, one of which is Python!

The '{r}' can be replaced with, for example '{python}'. Knit willr then use the specified language engine to execute the code!

An issue here is that it runs the code as a separate enviroment; you can't pass things from a Python chunk directly into another chunk, so you have to send it via the file system. Here I just output CSVs; you could also try out [feather](https://blog.rstudio.org/2016/03/29/feather/), which will probably be faster.

```{r plotCleanVsNot, warning = FALSE, message = FALSE, fig.align = "center"}
library(tidyverse)
library(ggplot2)
library(scales)
library(grid)
library(gridExtra)

clean_data <- read_csv('C:\\Users\\Matt\\Documents\\Github\\ExploringERPs\\evoked_clean.csv') %>%
  gather(electrode, amplitude,-time)
unclean_data <- read_csv('C:\\Users\\Matt\\Documents\\Github\\ExploringERPs\\evoked_preclean.csv') %>%
  gather(electrode,amplitude,-time)

theme_set(theme_bw())

unclean.plot <- ggplot(unclean_data,aes(time,amplitude))+
  geom_line(aes(colour = electrode))+ggtitle('Before artefact rejection')+guides(colour = FALSE)

clean.plot <- ggplot(clean_data,aes(time,amplitude))+
  geom_line(aes(colour = electrode))+ggtitle('After artefact rejection')+guides(colour = FALSE)

grid.arrange(unclean.plot,clean.plot)
```

```{r plot_topgraphy, echo = FALSE, message = FALSE, warning = FALSE, fig.height= 4, fig.width = 4, fig.align = "center", fig.cap = "ERPs and a topography at 172 ms after stimulus onset"}
electrodeLocs <- read_delim("https://raw.githubusercontent.com/craddm/ExploringERPs/master/biosemi70elecs.loc", "\t", escape_double = FALSE, col_names = c("chanNo","theta","radius","electrode"), trim_ws = TRUE)
electrodeLocs$radianTheta <- pi/180*electrodeLocs$theta
electrodeLocs <- electrodeLocs %>%
  mutate(x = .$radius*sin(.$radianTheta),
         y = .$radius*cos(.$radianTheta))

theme_topo <- function(base_size = 12)
  {
  theme_bw(base_size = base_size) %+replace%
      theme(
            rect             = element_blank(),
            line             = element_blank(),
            axis.text = element_blank(),
            axis.title = element_blank()
           )
}

circleFun <- function(center = c(0,0),diameter = 1, npoints = 100) {
  r = diameter / 2
  tt <- seq(0,2*pi,length.out = npoints)
  xx <- center[1] + r * cos(tt)
  yy <- center[2] + r * sin(tt)
  return(data.frame(x = xx, y = yy))
}

headShape <- circleFun(c(0, 0), round(max(electrodeLocs$x)), npoints = 100) # 0
nose <- data.frame(x = c(-0.05,0,.05),y=c(.495,.55,.495))
allData <- clean_data %>% left_join(electrodeLocs, by = "electrode")

jet.colors <-
  colorRampPalette(c("#00007F", "blue", "#007FFF", "cyan",
                     "#7FFF7F", "yellow", "#FF7F00", "red", "#7F0000"))

seismic.colors <- colorRampPalette(c("#00004C","#0000FF","#A1A1FF","white","#FFA1A1","red","#7F0000"),interpolate = "spline")

matplotlibRdBu_r <- colorRampPalette(c("#053061","#4694C4","#F6F6F6","#E7886C","#67001F"),interpolate = "spline")

singleTimepoint <- filter(allData,time == 172)
rmax <- .75   #specify a maximum boundary for the grid
gridRes <- 67 #specify the interpolation grid resolution

## Create a function to perform Matlab's v4 interpolation.
## Takes as input a data-frame with columns x, y, and z (x co-ordinates, y co-ordinates, and amplitude)
## and variables xo and yo, the co-ordinates which will be use to create a grid for interpolation

v4Interp <- function(df,xo,yo) {
  xo <- matrix(rep(xo,length(yo)),nrow = length(xo),ncol = length(yo))
  yo <- t(matrix(rep(yo,length(xo)),nrow = length(yo),ncol = length(xo)))
  xy <- df$x + df$y*sqrt(as.complex(-1))
  d <- matrix(rep(xy,length(xy)),nrow = length(xy), ncol = length(xy))
  d <- abs(d - t(d))
  diag(d) <- 1
  g <- (d^2) * (log(d)-1)   # Green's function.
  diag(g) <- 0
  weights <- qr.solve(g,df$z)
  xy <- t(xy)
  outmat <- matrix(nrow = gridRes,ncol = gridRes)
  for (i in 1:gridRes){
    for (j in 1:gridRes) {
      test4 <- abs((xo[i,j] + sqrt(as.complex(-1))*yo[i,j]) - xy)
      g <- (test4^2) * (log(test4)-1)
      outmat[i,j] <- g %*% weights
      }
    }
  return(list(x = xo[,1],y = yo[1,],z = outmat))
}

## Create data frame to be used for interpolation
testDat<- data.frame(x = singleTimepoint$x,
                     y = singleTimepoint$y,
                     z = singleTimepoint$amplitude)

#Create the interpolation grid
xo <- seq(min(-rmax, testDat$x), max(rmax, testDat$x), length = gridRes)
yo <- seq(max(rmax, testDat$y), min(-rmax, testDat$y), length = gridRes)

finalOut <- v4Interp(testDat, xo, yo)

interpV4 <- data.frame(x = finalOut$x,
                       finalOut$z)
names(interpV4)[1:length(finalOut$y)+1] <- finalOut$y

interpV4 <- gather(interpV4,
                   key = y,
                   value = amplitude,
                   -x,
                   convert = TRUE) 

interpV4$incircle <- (interpV4$x)^2 + (interpV4$y)^2 < 0.7 ^ 2 # mark
maskRing <- circleFun(diameter = 1.42) 
v4plot <- ggplot(interpV4[interpV4$incircle,],aes(x = x, y = y, fill = amplitude))+
  geom_raster(interpolate = TRUE)+
  stat_contour(aes(z = amplitude), colour = "black",binwidth = 1)+
  theme_topo()+
   geom_path(data = maskRing,
            aes(x, y, z = NULL, fill =NULL),
            colour = "white",
            size = 6)+
  scale_fill_gradientn(colours = jet.colors(10),
                       limits = c(-5,5),
                       guide = "colourbar",
                       oob = squish)+
  geom_point(data = singleTimepoint,
             aes(x,y),
             size = 1)+
  geom_path(data = headShape,
            aes(x,y,z = NULL,fill = NULL),
            size = 1.5)+
  geom_path(data = nose,
            aes(x, y, z = NULL, fill = NULL),
            size = 1.5)+
  coord_equal()
v4plot + scale_fill_distiller(type = "div",
                              palette = "RdBu",
                              #limits = c(-5,5),
                              guide = "colourbar",
                              oob = squish)
```

Of course, why do this? Personally, I'm still just getting to grips with Python and MNE, and some of the analyses I want to do aren't so easy to do in Python (at least not for me, not yet). This is as much a demonstration of the fact that you are not tied to using one language as anything else. You can use multiple languages in combination when you need to, even simultaneously. You can also call R code from Python using, for example, [RPy2](http://blog.yhat.com/posts/rpy2-combing-the-power-of-r-and-python.html)

Another alternative to running Python in code chunks, as done here, is to use the recently released [reticulate](https://rdrr.io/cran/reticulate/man/reticulate.html) package for R, which allows you to run Python code directly within R. So you'd be able to cut out the middleman and not write to disk, as I had to above.